{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled6.ipynb","provenance":[],"mount_file_id":"1minf3DszLfvJHxOdp1jdMf337UPuPr62","authorship_tag":"ABX9TyOPT8KQNEBSpfdpcCSat40/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"2OzL6nxMaMxk","executionInfo":{"status":"ok","timestamp":1601544259836,"user_tz":-330,"elapsed":10802,"user":{"displayName":"Shashank._.d","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEehjcFjPDG9QrTE6vYUYL9Gp09bMFfsSMzX1PHw=s64","userId":"14108647885313081555"}},"outputId":"a5f6aafa-41a0-4ff4-9401-6811eadb5c94","colab":{"base_uri":"https://localhost:8080/","height":765}},"source":[" \n","import string\n"," \n","# load doc into memory\n","def load_doc(filename):\n","    # open the file as read only\n","    file = open(filename, 'r')\n","    # read all text\n","    text = file.read()\n","    # close the file\n","    file.close()\n","    return text\n"," \n","# turn a doc into clean tokens\n","def clean_doc(doc):\n","    # replace '--' with a space ' '\n","    doc = doc.replace('--', ' ')\n","    # split into tokens by white space\n","    tokens = doc.split()\n","    # remove punctuation from each token\n","    table = str.maketrans('', '', string.punctuation)\n","    tokens = [w.translate(table) for w in tokens]\n","    # remove remaining tokens that are not alphabetic\n","    tokens = [word for word in tokens if word.isalpha()]\n","    # make lower case\n","    tokens = [word.lower() for word in tokens]\n","    return tokens\n"," \n","# save tokens to file, one dialog per line\n","def save_doc(lines, filename):\n","    data = '\\n'.join(lines)\n","    file = open(filename, 'w')\n","    file.write(data)\n","    file.close()\n"," \n","# load document\n","in_filename = '/content/drive/My Drive/python-386-docs.TXT'\n","doc = load_doc(in_filename)\n","print(doc[:200])\n"," \n","# clean document\n","tokens = clean_doc(doc)\n","print(tokens[:200])\n","print('Total Tokens: %d' % len(tokens))\n","print('Unique Tokens: %d' % len(set(tokens)))\n"," \n","# organize into sequences of tokens\n","length = 50 + 1\n","sequences = list()\n","for i in range(length, len(tokens)):\n","    # select sequence of tokens\n","    seq = tokens[i-length:i]\n","    # convert into a line\n","    line = ' '.join(seq)\n","    # store\n","    sequences.append(line)\n","print('Total Sequences: %d' % len(sequences))\n"," \n","# save sequences to file\n","out_filename = 'republic_sequences.txt'\n","save_doc(sequences, out_filename)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["\n","Other Language Changes\n","\n","New Modules\n","\n","Improved Modules ast\n","\n","asyncio\n","\n","builtins\n","\n","collections\n","\n","cProfile\n","\n","csv\n","\n","curses\n","\n","ctypes\n","\n","datetime\n","\n","functools\n","\n","gc\n","\n","gettext\n","\n","gzip\n","\n","IDLE and idlelib\n","\n","inspect\n","\n","io\n","\n","iterto\n","['other', 'language', 'changes', 'new', 'modules', 'improved', 'modules', 'ast', 'asyncio', 'builtins', 'collections', 'cprofile', 'csv', 'curses', 'ctypes', 'datetime', 'functools', 'gc', 'gettext', 'gzip', 'idle', 'and', 'idlelib', 'inspect', 'io', 'itertools', 'jsontool', 'logging', 'math', 'mmap', 'multiprocessing', 'os', 'ospath', 'pathlib', 'pickle', 'plistlib', 'pprint', 'pycompile', 'shlex', 'shutil', 'socket', 'ssl', 'statistics', 'sys', 'tarfile', 'threading', 'tokenize', 'tkinter', 'time', 'typing', 'unicodedata', 'unittest', 'venv', 'weakref', 'xml', 'xmlrpc', 'optimizations', 'build', 'and', 'c', 'api', 'changes', 'deprecated', 'api', 'and', 'feature', 'removals', 'porting', 'to', 'python', 'changes', 'in', 'python', 'behavior', 'changes', 'in', 'the', 'python', 'api', 'changes', 'in', 'the', 'c', 'api', 'cpython', 'bytecode', 'changes', 'demos', 'and', 'tools', 'notable', 'changes', 'in', 'python', 'notable', 'changes', 'in', 'python', 'notable', 'changes', 'in', 'python', 'new', 'in', 'python', 'summary', 'release', 'highlights', 'new', 'features', 'pep', 'postponed', 'evaluation', 'of', 'annotations', 'pep', 'legacy', 'c', 'locale', 'coercion', 'pep', 'forced', 'runtime', 'mode', 'pep', 'builtin', 'breakpoint', 'pep', 'new', 'c', 'api', 'for', 'threadlocal', 'storage', 'pep', 'customization', 'of', 'access', 'to', 'module', 'attributes', 'pep', 'new', 'time', 'functions', 'with', 'nanosecond', 'resolution', 'pep', 'show', 'deprecationwarning', 'in', 'main', 'pep', 'core', 'support', 'for', 'typing', 'module', 'and', 'generic', 'types', 'pep', 'hashbased', 'pyc', 'files', 'pep', 'python', 'documentation', 'translations', 'development', 'runtime', 'mode', 'x', 'dev', 'other', 'language', 'changes', 'new', 'modules', 'contextvars', 'dataclasses', 'importlibresources', 'improved', 'modules', 'argparse', 'asyncio', 'binascii', 'calendar', 'collections', 'compileall', 'concurrentfutures', 'contextlib', 'cprofile', 'crypt', 'datetime', 'dbm', 'decimal', 'dis', 'distutils']\n","Total Tokens: 1328720\n","Unique Tokens: 51576\n","Total Sequences: 1328669\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9d_iQRUEbICQ"},"source":[" import tensorflow as tf\n","from numpy import array\n","from pickle import dump\n","from keras.preprocessing.text import Tokenizer\n","from keras.utils import to_categorical\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import LSTM\n","from keras.layers import Embedding\n"," \n","# load doc into memory\n","def load_doc(filename):\n","    # open the file as read only\n","    file = open(filename, 'r')\n","    # read all text\n","    text = file.read()\n","    # close the file\n","    file.close()\n","    return text\n"," \n","# load\n","in_filename = 'republic_sequences.txt'\n","doc = load_doc(in_filename)\n","lines = doc.split('\\n')\n"," \n","# integer encode sequences of words\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(lines)\n","sequences = tokenizer.texts_to_sequences(lines)\n","# vocabulary size\n","vocab_size = len(tokenizer.word_index) + 1\n"," \n"," \n"," \n"," \n"," \n","# separate into input and output\n","sequences = array(sequences)\n","X, y = sequences[:,:-1], sequences[:,-1]\n","y = to_categorical(y, num_classes=vocab_size)\n","seq_length = X.shape[1]\n"," \n","# define model\n","model = Sequential()\n","model.add(Embedding(vocab_size, 50, input_length=seq_length))\n","model.add(LSTM(100, return_sequences=True))\n","model.add(LSTM(100))\n","model.add(Dense(100, activation='relu'))\n","model.add(Dense(vocab_size, activation='softmax'))\n","print(model.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I7JZfZJVdnsv"},"source":["resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n","tf.config.experimental_connect_to_cluster(resolver)\n","# This is the TPU initialization code that has to be at the beginning.\n","tf.tpu.experimental.initialize_tpu_system(resolver)\n","print(\"All devices: \", tf.config.list_logical_devices('TPU'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-KRGPeNLdqi0"},"source":["strategy = tf.distribute.experimental.TPUStrategy(resolver)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j95yrkANdEi3"},"source":[" \n","with strategy.scope():\n","    # compile model\n","    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    # fit model\n","    model.fit(X, y, batch_size=128, epochs=100)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LWWD2zMDdJ6L"},"source":[" \n","# save the model to file\n","model.save('model.h5')\n","# save the tokenizer\n","dump(tokenizer, open('tokenizer.pkl', 'wb'))"],"execution_count":null,"outputs":[]}]}